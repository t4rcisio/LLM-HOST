
# ğŸ§  API OLLAMA HOST

Uma API RESTful construÃ­da com **FastAPI** para hospedar e interagir com modelos via **Ollama**, com suporte a chamadas sÃ­ncronas, assÃ­ncronas (streaming) e registro de uso.

---

## ğŸ“¦ Requisitos

- Python 3.13.5+
- pip (gerenciador de pacotes Python)
- `ollama` instalado e rodando localmente
- DependÃªncias listadas no `requirements.txt`

---

1. Crie um ambiente virtual e ative-o:

```bash
python -m venv venv
source venv/bin/activate     # Linux/macOS
venv\Scripts\activate      # Windows
```

2. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ ExecuÃ§Ã£o

Execute o servidor FastAPI com:

```bash
python .\main.py
```


Execute o servidor FastAPI como serviÃ§o com:

```bash
python .\launcher.py
```

A aplicaÃ§Ã£o serÃ¡ iniciada em:

```
http://127.0.0.1:5000
```

A documentaÃ§Ã£o Swagger estarÃ¡ disponÃ­vel em:

```
http://127.0.0.1:5000/docs
```

Ou a documentaÃ§Ã£o ReDoc:

```
http://127.0.0.1:5000/redoc
```

---

## ğŸ“š Endpoints

### ğŸ” /api/v1/ai/models [GET]

Retorna a lista de modelos Ollama disponÃ­veis no host.

**Exemplo de resposta:**
```json
["llama3", "qwen", "mistral", "custom-model"]
```

---

### ğŸ’¬ /api/v1/ai/ask_sync [POST]

Envia uma pergunta e aguarda a resposta completa (modo sÃ­ncrono).

**Body JSON:**
```json
{
  "agent": "llama3",
  "content": "Explique a teoria da relatividade.",
  "template": "VocÃª Ã© um assistente educado e tÃ©cnico.",
  "user_key": "111111122",
  "project": "projeto1",
  "process_id": "001",
  "subprocess_id": "001"
}
```
user_key, project, process_id, subprocess_id sÃ£o parÃ¢metros opcionais.

**Resposta:**
```text
Texto completo da resposta gerada pelo modelo.
```

---

### ğŸ“¡ /api/v1/ai/ask_async [POST]

Envia uma pergunta com resposta em streaming (modo assÃ­ncrono).

**Mesmo body JSON** do endpoint `/ask_sync`.

A resposta serÃ¡ transmitida em partes (`text/plain`) Ã  medida que for gerada.

---

### ğŸ“Š /api/v1/admin/dashboard [GET]

Retorna os logs de uso dos modelos, incluindo tempo de resposta, tokens e identificadores de projeto/processo.

**Exemplo de resposta:**
```json
[
  {
    "date": "2025-08-04T11:05:00",
    "total_seconds": 1500000000,
    "total_tokens": 123,
    "input_tokens": 23,
    "output_tokens": 100,
    "model_name": "qwen",
    "input": [{"role": "user", "content": "exemplo"}],
    "output": "resposta gerada",
    "project": "projeto1",
    "process_id": "p001",
    "subprocess_id": "s001"
  }
]
```

---

## ğŸ›  Estrutura do Projeto

- `main.py`: ponto de entrada da aplicaÃ§Ã£o
- `requirements.txt`: dependÃªncias do projeto
- `core/`: mÃ³dulo com funÃ§Ãµes utilitÃ¡rias, armazenamento e controle de uso
- `schema/`: mÃ³dulo com os esquemas de input e output da api
- `api/`: mÃ³dulo com a configuraÃ§Ã£o dos endpoints

---

## ğŸ‘¤ Autor

tarcisio.b.prates